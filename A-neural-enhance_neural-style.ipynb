{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Neural-Enhance from Docker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!enhance input/640.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!th neural_style/neural_style.lua -style_image images/pitt.jpg -content_image images/out65.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker-cpu.df  docs\t   LICENSE     requirements.txt\r\n",
      "docker-gpu.df  enhance.py  README.rst  train\r\n"
     ]
    }
   ],
   "source": [
    "!ls neural-enhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load neural_style/neural_style.lua\n",
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'image'\n",
    "require 'optim'\n",
    "\n",
    "require 'loadcaffe'\n",
    "\n",
    "\n",
    "local cmd = torch.CmdLine()\n",
    "\n",
    "-- Basic options\n",
    "cmd:option('-style_image', 'examples/inputs/seated-nude.jpg',\n",
    "           'Style target image')\n",
    "cmd:option('-style_blend_weights', 'nil')\n",
    "cmd:option('-content_image', 'examples/inputs/tubingen.jpg',\n",
    "           'Content target image')\n",
    "cmd:option('-image_size', 350, 'Maximum height / width of generated image')\n",
    "cmd:option('-gpu', '-1', 'Zero-indexed ID of the GPU to use; for CPU mode set -gpu = -1')\n",
    "cmd:option('-multigpu_strategy', '', 'Index of layers to split the network across GPUs')\n",
    "\n",
    "-- Optimization options\n",
    "cmd:option('-content_weight', 5e0)\n",
    "cmd:option('-style_weight', 1e2)\n",
    "cmd:option('-tv_weight', 1e-3)\n",
    "cmd:option('-num_iterations', 1000)\n",
    "cmd:option('-normalize_gradients', false)\n",
    "cmd:option('-init', 'random', 'random|image')\n",
    "cmd:option('-init_image', '')\n",
    "cmd:option('-optimizer', 'adam', 'lbfgs|adam')\n",
    "cmd:option('-learning_rate', 1e1)\n",
    "cmd:option('-lbfgs_num_correction', 0)\n",
    "\n",
    "-- Output options\n",
    "cmd:option('-print_iter', 1)\n",
    "cmd:option('-save_iter', 5)\n",
    "cmd:option('-output_image', 'out.png')\n",
    "\n",
    "-- Other options\n",
    "cmd:option('-style_scale', 1.0)\n",
    "cmd:option('-original_colors', 0)\n",
    "cmd:option('-pooling', 'max', 'max|avg')\n",
    "cmd:option('-proto_file', 'models/VGG_ILSVRC_19_layers_deploy.prototxt')\n",
    "cmd:option('-model_file', 'models/VGG_ILSVRC_19_layers.caffemodel')\n",
    "cmd:option('-backend', 'nn', 'nn|cudnn|clnn')\n",
    "cmd:option('-cudnn_autotune', false)\n",
    "cmd:option('-seed', -1)\n",
    "\n",
    "cmd:option('-content_layers', 'relu4_2', 'layers for content')\n",
    "cmd:option('-style_layers', 'relu1_1,relu2_1,relu3_1,relu4_1,relu5_1', 'layers for style')\n",
    "\n",
    "\n",
    "local function main(params)\n",
    "  local dtype, multigpu = setup_gpu(params)\n",
    "\n",
    "  local loadcaffe_backend = params.backend\n",
    "  if params.backend == 'clnn' then loadcaffe_backend = 'nn' end\n",
    "  local cnn = loadcaffe.load(params.proto_file, params.model_file, loadcaffe_backend):type(dtype)\n",
    "\n",
    "  local content_image = image.load(params.content_image, 3)\n",
    "  content_image = image.scale(content_image, params.image_size, 'bilinear')\n",
    "  local content_image_caffe = preprocess(content_image):float()\n",
    "\n",
    "  local style_size = math.ceil(params.style_scale * params.image_size)\n",
    "  local style_image_list = params.style_image:split(',')\n",
    "  local style_images_caffe = {}\n",
    "  for _, img_path in ipairs(style_image_list) do\n",
    "    local img = image.load(img_path, 3)\n",
    "    img = image.scale(img, style_size, 'bilinear')\n",
    "    local img_caffe = preprocess(img):float()\n",
    "    table.insert(style_images_caffe, img_caffe)\n",
    "  end\n",
    "\n",
    "  local init_image = nil\n",
    "  if params.init_image ~= '' then\n",
    "    init_image = image.load(params.init_image, 3)\n",
    "    local H, W = content_image:size(2), content_image:size(3)\n",
    "    init_image = image.scale(init_image, W, H, 'bilinear')\n",
    "    init_image = preprocess(init_image):float()\n",
    "  end\n",
    "\n",
    "  -- Handle style blending weights for multiple style inputs\n",
    "  local style_blend_weights = nil\n",
    "  if params.style_blend_weights == 'nil' then\n",
    "    -- Style blending not specified, so use equal weighting\n",
    "    style_blend_weights = {}\n",
    "    for i = 1, #style_image_list do\n",
    "      table.insert(style_blend_weights, 1.0)\n",
    "    end\n",
    "  else\n",
    "    style_blend_weights = params.style_blend_weights:split(',')\n",
    "    assert(#style_blend_weights == #style_image_list,\n",
    "      '-style_blend_weights and -style_images must have the same number of elements')\n",
    "  end\n",
    "  -- Normalize the style blending weights so they sum to 1\n",
    "  local style_blend_sum = 0\n",
    "  for i = 1, #style_blend_weights do\n",
    "    style_blend_weights[i] = tonumber(style_blend_weights[i])\n",
    "    style_blend_sum = style_blend_sum + style_blend_weights[i]\n",
    "  end\n",
    "  for i = 1, #style_blend_weights do\n",
    "    style_blend_weights[i] = style_blend_weights[i] / style_blend_sum\n",
    "  end\n",
    "\n",
    "  local content_layers = params.content_layers:split(\",\")\n",
    "  local style_layers = params.style_layers:split(\",\")\n",
    "\n",
    "  -- Set up the network, inserting style and content loss modules\n",
    "  local content_losses, style_losses = {}, {}\n",
    "  local next_content_idx, next_style_idx = 1, 1\n",
    "  local net = nn.Sequential()\n",
    "  if params.tv_weight > 0 then\n",
    "    local tv_mod = nn.TVLoss(params.tv_weight):type(dtype)\n",
    "    net:add(tv_mod)\n",
    "  end\n",
    "  for i = 1, #cnn do\n",
    "    if next_content_idx <= #content_layers or next_style_idx <= #style_layers then\n",
    "      local layer = cnn:get(i)\n",
    "      local name = layer.name\n",
    "      local layer_type = torch.type(layer)\n",
    "      local is_pooling = (layer_type == 'cudnn.SpatialMaxPooling' or layer_type == 'nn.SpatialMaxPooling')\n",
    "      if is_pooling and params.pooling == 'avg' then\n",
    "        assert(layer.padW == 0 and layer.padH == 0)\n",
    "        local kW, kH = layer.kW, layer.kH\n",
    "        local dW, dH = layer.dW, layer.dH\n",
    "        local avg_pool_layer = nn.SpatialAveragePooling(kW, kH, dW, dH):type(dtype)\n",
    "        local msg = 'Replacing max pooling at layer %d with average pooling'\n",
    "        print(string.format(msg, i))\n",
    "        net:add(avg_pool_layer)\n",
    "      else\n",
    "        net:add(layer)\n",
    "      end\n",
    "      if name == content_layers[next_content_idx] then\n",
    "        print(\"Setting up content layer\", i, \":\", layer.name)\n",
    "        local norm = params.normalize_gradients\n",
    "        local loss_module = nn.ContentLoss(params.content_weight, norm):type(dtype)\n",
    "        net:add(loss_module)\n",
    "        table.insert(content_losses, loss_module)\n",
    "        next_content_idx = next_content_idx + 1\n",
    "      end\n",
    "      if name == style_layers[next_style_idx] then\n",
    "        print(\"Setting up style layer  \", i, \":\", layer.name)\n",
    "        local norm = params.normalize_gradients\n",
    "        local loss_module = nn.StyleLoss(params.style_weight, norm):type(dtype)\n",
    "        net:add(loss_module)\n",
    "        table.insert(style_losses, loss_module)\n",
    "        next_style_idx = next_style_idx + 1\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  if multigpu then\n",
    "    net = setup_multi_gpu(net, params)\n",
    "  end\n",
    "  net:type(dtype)\n",
    "\n",
    "  -- Capture content targets\n",
    "  for i = 1, #content_losses do\n",
    "    content_losses[i].mode = 'capture'\n",
    "  end\n",
    "  print 'Capturing content targets'\n",
    "  print(net)\n",
    "  content_image_caffe = content_image_caffe:type(dtype)\n",
    "  net:forward(content_image_caffe:type(dtype))\n",
    "\n",
    "  -- Capture style targets\n",
    "  for i = 1, #content_losses do\n",
    "    content_losses[i].mode = 'none'\n",
    "  end\n",
    "  for i = 1, #style_images_caffe do\n",
    "    print(string.format('Capturing style target %d', i))\n",
    "    for j = 1, #style_losses do\n",
    "      style_losses[j].mode = 'capture'\n",
    "      style_losses[j].blend_weight = style_blend_weights[i]\n",
    "    end\n",
    "    net:forward(style_images_caffe[i]:type(dtype))\n",
    "  end\n",
    "\n",
    "  -- Set all loss modules to loss mode\n",
    "  for i = 1, #content_losses do\n",
    "    content_losses[i].mode = 'loss'\n",
    "  end\n",
    "  for i = 1, #style_losses do\n",
    "    style_losses[i].mode = 'loss'\n",
    "  end\n",
    "\n",
    "  -- We don't need the base CNN anymore, so clean it up to save memory.\n",
    "  cnn = nil\n",
    "  for i=1, #net.modules do\n",
    "    local module = net.modules[i]\n",
    "    if torch.type(module) == 'nn.SpatialConvolutionMM' then\n",
    "        -- remove these, not used, but uses gpu memory\n",
    "        module.gradWeight = nil\n",
    "        module.gradBias = nil\n",
    "    end\n",
    "  end\n",
    "  collectgarbage()\n",
    "\n",
    "  -- Initialize the image\n",
    "  if params.seed >= 0 then\n",
    "    torch.manualSeed(params.seed)\n",
    "  end\n",
    "  local img = nil\n",
    "  if params.init == 'random' then\n",
    "    img = torch.randn(content_image:size()):float():mul(0.001)\n",
    "  elseif params.init == 'image' then\n",
    "    if init_image then\n",
    "      img = init_image:clone()\n",
    "    else\n",
    "      img = content_image_caffe:clone()\n",
    "    end\n",
    "  else\n",
    "    error('Invalid init type')\n",
    "  end\n",
    "  img = img:type(dtype)\n",
    "\n",
    "  -- Run it through the network once to get the proper size for the gradient\n",
    "  -- All the gradients will come from the extra loss modules, so we just pass\n",
    "  -- zeros into the top of the net on the backward pass.\n",
    "  local y = net:forward(img)\n",
    "  local dy = img.new(#y):zero()\n",
    "\n",
    "  -- Declaring this here lets us access it in maybe_print\n",
    "  local optim_state = nil\n",
    "  if params.optimizer == 'lbfgs' then\n",
    "    optim_state = {\n",
    "      maxIter = params.num_iterations,\n",
    "      verbose=true,\n",
    "      tolX=-1,\n",
    "      tolFun=-1,\n",
    "    }\n",
    "    if params.lbfgs_num_correction > 0 then\n",
    "      optim_state.nCorrection = params.lbfgs_num_correction\n",
    "    end\n",
    "  elseif params.optimizer == 'adam' then\n",
    "    optim_state = {\n",
    "      learningRate = params.learning_rate,\n",
    "    }\n",
    "  else\n",
    "    error(string.format('Unrecognized optimizer \"%s\"', params.optimizer))\n",
    "  end\n",
    "\n",
    "  local function maybe_print(t, loss)\n",
    "    local verbose = (params.print_iter > 0 and t % params.print_iter == 0)\n",
    "    if verbose then\n",
    "      print(string.format('Iteration %d / %d', t, params.num_iterations))\n",
    "      for i, loss_module in ipairs(content_losses) do\n",
    "        print(string.format('  Content %d loss: %f', i, loss_module.loss))\n",
    "      end\n",
    "      for i, loss_module in ipairs(style_losses) do\n",
    "        print(string.format('  Style %d loss: %f', i, loss_module.loss))\n",
    "      end\n",
    "      print(string.format('  Total loss: %f', loss))\n",
    "    end\n",
    "  end\n",
    "\n",
    "  local function maybe_save(t)\n",
    "    local should_save = params.save_iter > 0 and t % params.save_iter == 0\n",
    "    should_save = should_save or t == params.num_iterations\n",
    "    if should_save then\n",
    "      local disp = deprocess(img:double())\n",
    "      disp = image.minmax{tensor=disp, min=0, max=1}\n",
    "      local filename = build_filename(params.output_image, t)\n",
    "      if t == params.num_iterations then\n",
    "        filename = params.output_image\n",
    "      end\n",
    "\n",
    "      -- Maybe perform postprocessing for color-independent style transfer\n",
    "      if params.original_colors == 1 then\n",
    "        disp = original_colors(content_image, disp)\n",
    "      end\n",
    "\n",
    "      image.save(filename, disp)\n",
    "    end\n",
    "  end\n",
    "\n",
    "  -- Function to evaluate loss and gradient. We run the net forward and\n",
    "  -- backward to get the gradient, and sum up losses from the loss modules.\n",
    "  -- optim.lbfgs internally handles iteration and calls this function many\n",
    "  -- times, so we manually count the number of iterations to handle printing\n",
    "  -- and saving intermediate results.\n",
    "  local num_calls = 0\n",
    "  local function feval(x)\n",
    "    num_calls = num_calls + 1\n",
    "    net:forward(x)\n",
    "    local grad = net:updateGradInput(x, dy)\n",
    "    local loss = 0\n",
    "    for _, mod in ipairs(content_losses) do\n",
    "      loss = loss + mod.loss\n",
    "    end\n",
    "    for _, mod in ipairs(style_losses) do\n",
    "      loss = loss + mod.loss\n",
    "    end\n",
    "    maybe_print(num_calls, loss)\n",
    "    maybe_save(num_calls)\n",
    "\n",
    "    collectgarbage()\n",
    "    -- optim.lbfgs expects a vector for gradients\n",
    "    return loss, grad:view(grad:nElement())\n",
    "  end\n",
    "\n",
    "  -- Run optimization.\n",
    "  if params.optimizer == 'lbfgs' then\n",
    "    print('Running optimization with L-BFGS')\n",
    "    local x, losses = optim.lbfgs(feval, img, optim_state)\n",
    "  elseif params.optimizer == 'adam' then\n",
    "    print('Running optimization with ADAM')\n",
    "    for t = 1, params.num_iterations do\n",
    "      local x, losses = optim.adam(feval, img, optim_state)\n",
    "    end\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "function setup_gpu(params)\n",
    "  local multigpu = false\n",
    "  if params.gpu:find(',') then\n",
    "    multigpu = true\n",
    "    params.gpu = params.gpu:split(',')\n",
    "    for i = 1, #params.gpu do\n",
    "      params.gpu[i] = tonumber(params.gpu[i]) + 1\n",
    "    end\n",
    "  else\n",
    "    params.gpu = tonumber(params.gpu) + 1\n",
    "  end\n",
    "  local dtype = 'torch.FloatTensor'\n",
    "  if multigpu or params.gpu > 0 then\n",
    "    if params.backend ~= 'clnn' then\n",
    "      require 'cutorch'\n",
    "      require 'cunn'\n",
    "      if multigpu then\n",
    "        cutorch.setDevice(params.gpu[1])\n",
    "      else\n",
    "        cutorch.setDevice(params.gpu)\n",
    "      end\n",
    "      dtype = 'torch.CudaTensor'\n",
    "    else\n",
    "      require 'clnn'\n",
    "      require 'cltorch'\n",
    "      if multigpu then\n",
    "        cltorch.setDevice(params.gpu[1])\n",
    "      else\n",
    "        cltorch.setDevice(params.gpu)\n",
    "      end\n",
    "      dtype = torch.Tensor():cl():type()\n",
    "    end\n",
    "  else\n",
    "    params.backend = 'nn'\n",
    "  end\n",
    "\n",
    "  if params.backend == 'cudnn' then\n",
    "    require 'cudnn'\n",
    "    if params.cudnn_autotune then\n",
    "      cudnn.benchmark = true\n",
    "    end\n",
    "    cudnn.SpatialConvolution.accGradParameters = nn.SpatialConvolutionMM.accGradParameters -- ie: nop\n",
    "  end\n",
    "  return dtype, multigpu\n",
    "end\n",
    "\n",
    "\n",
    "function setup_multi_gpu(net, params)\n",
    "  local DEFAULT_STRATEGIES = {\n",
    "    [2] = {3},\n",
    "  }\n",
    "  local gpu_splits = nil\n",
    "  if params.multigpu_strategy == '' then\n",
    "    -- Use a default strategy\n",
    "    gpu_splits = DEFAULT_STRATEGIES[#params.gpu]\n",
    "    -- Offset the default strategy by one if we are using TV\n",
    "    if params.tv_weight > 0 then\n",
    "      for i = 1, #gpu_splits do gpu_splits[i] = gpu_splits[i] + 1 end\n",
    "    end\n",
    "  else\n",
    "    -- Use the user-specified multigpu strategy\n",
    "    gpu_splits = params.multigpu_strategy:split(',')\n",
    "    for i = 1, #gpu_splits do\n",
    "      gpu_splits[i] = tonumber(gpu_splits[i])\n",
    "    end\n",
    "  end\n",
    "  assert(gpu_splits ~= nil, 'Must specify -multigpu_strategy')\n",
    "  local gpus = params.gpu\n",
    "\n",
    "  local cur_chunk = nn.Sequential()\n",
    "  local chunks = {}\n",
    "  for i = 1, #net do\n",
    "    cur_chunk:add(net:get(i))\n",
    "    if i == gpu_splits[1] then\n",
    "      table.remove(gpu_splits, 1)\n",
    "      table.insert(chunks, cur_chunk)\n",
    "      cur_chunk = nn.Sequential()\n",
    "    end\n",
    "  end\n",
    "  table.insert(chunks, cur_chunk)\n",
    "  assert(#chunks == #gpus)\n",
    "\n",
    "  local new_net = nn.Sequential()\n",
    "  for i = 1, #chunks do\n",
    "    local out_device = nil\n",
    "    if i == #chunks then\n",
    "      out_device = gpus[1]\n",
    "    end\n",
    "    new_net:add(nn.GPU(chunks[i], gpus[i], out_device))\n",
    "  end\n",
    "\n",
    "  return new_net\n",
    "end\n",
    "\n",
    "\n",
    "function build_filename(output_image, iteration)\n",
    "  local ext = paths.extname(output_image)\n",
    "  local basename = paths.basename(output_image, ext)\n",
    "  local directory = paths.dirname(output_image)\n",
    "  return string.format('%s/%s_%04d.%s',directory, basename, iteration, ext)\n",
    "end\n",
    "\n",
    "\n",
    "-- Preprocess an image before passing it to a Caffe model.\n",
    "-- We need to rescale from [0, 1] to [0, 255], convert from RGB to BGR,\n",
    "-- and subtract the mean pixel.\n",
    "function preprocess(img)\n",
    "  local mean_pixel = torch.DoubleTensor({103.939, 116.779, 123.68})\n",
    "  local perm = torch.LongTensor{3, 2, 1}\n",
    "  img = img:index(1, perm):mul(256.0)\n",
    "  mean_pixel = mean_pixel:view(3, 1, 1):expandAs(img)\n",
    "  img:add(-1, mean_pixel)\n",
    "  return img\n",
    "end\n",
    "\n",
    "\n",
    "-- Undo the above preprocessing.\n",
    "function deprocess(img)\n",
    "  local mean_pixel = torch.DoubleTensor({103.939, 116.779, 123.68})\n",
    "  mean_pixel = mean_pixel:view(3, 1, 1):expandAs(img)\n",
    "  img = img + mean_pixel\n",
    "  local perm = torch.LongTensor{3, 2, 1}\n",
    "  img = img:index(1, perm):div(256.0)\n",
    "  return img\n",
    "end\n",
    "\n",
    "\n",
    "-- Combine the Y channel of the generated image and the UV channels of the\n",
    "-- content image to perform color-independent style transfer.\n",
    "function original_colors(content, generated)\n",
    "  local generated_y = image.rgb2yuv(generated)[{{1, 1}}]\n",
    "  local content_uv = image.rgb2yuv(content)[{{2, 3}}]\n",
    "  return image.yuv2rgb(torch.cat(generated_y, content_uv, 1))\n",
    "end\n",
    "\n",
    "\n",
    "-- Define an nn Module to compute content loss in-place\n",
    "local ContentLoss, parent = torch.class('nn.ContentLoss', 'nn.Module')\n",
    "\n",
    "function ContentLoss:__init(strength, normalize)\n",
    "  parent.__init(self)\n",
    "  self.strength = strength\n",
    "  self.target = torch.Tensor()\n",
    "  self.normalize = normalize or false\n",
    "  self.loss = 0\n",
    "  self.crit = nn.MSECriterion()\n",
    "  self.mode = 'none'\n",
    "end\n",
    "\n",
    "function ContentLoss:updateOutput(input)\n",
    "  if self.mode == 'loss' then\n",
    "    self.loss = self.crit:forward(input, self.target) * self.strength\n",
    "  elseif self.mode == 'capture' then\n",
    "    self.target:resizeAs(input):copy(input)\n",
    "  end\n",
    "  self.output = input\n",
    "  return self.output\n",
    "end\n",
    "\n",
    "function ContentLoss:updateGradInput(input, gradOutput)\n",
    "  if self.mode == 'loss' then\n",
    "    if input:nElement() == self.target:nElement() then\n",
    "      self.gradInput = self.crit:backward(input, self.target)\n",
    "    end\n",
    "    if self.normalize then\n",
    "      self.gradInput:div(torch.norm(self.gradInput, 1) + 1e-8)\n",
    "    end\n",
    "    self.gradInput:mul(self.strength)\n",
    "    self.gradInput:add(gradOutput)\n",
    "  else\n",
    "    self.gradInput:resizeAs(gradOutput):copy(gradOutput)\n",
    "  end\n",
    "  return self.gradInput\n",
    "end\n",
    "\n",
    "\n",
    "local Gram, parent = torch.class('nn.GramMatrix', 'nn.Module')\n",
    "\n",
    "function Gram:__init()\n",
    "  parent.__init(self)\n",
    "end\n",
    "\n",
    "function Gram:updateOutput(input)\n",
    "  assert(input:dim() == 3)\n",
    "  local C, H, W = input:size(1), input:size(2), input:size(3)\n",
    "  local x_flat = input:view(C, H * W)\n",
    "  self.output:resize(C, C)\n",
    "  self.output:mm(x_flat, x_flat:t())\n",
    "  return self.output\n",
    "end\n",
    "\n",
    "function Gram:updateGradInput(input, gradOutput)\n",
    "  assert(input:dim() == 3 and input:size(1))\n",
    "  local C, H, W = input:size(1), input:size(2), input:size(3)\n",
    "  local x_flat = input:view(C, H * W)\n",
    "  self.gradInput:resize(C, H * W):mm(gradOutput, x_flat)\n",
    "  self.gradInput:addmm(gradOutput:t(), x_flat)\n",
    "  self.gradInput = self.gradInput:view(C, H, W)\n",
    "  return self.gradInput\n",
    "end\n",
    "\n",
    "\n",
    "-- Define an nn Module to compute style loss in-place\n",
    "local StyleLoss, parent = torch.class('nn.StyleLoss', 'nn.Module')\n",
    "\n",
    "function StyleLoss:__init(strength, normalize)\n",
    "  parent.__init(self)\n",
    "  self.normalize = normalize or false\n",
    "  self.strength = strength\n",
    "  self.target = torch.Tensor()\n",
    "  self.mode = 'none'\n",
    "  self.loss = 0\n",
    "\n",
    "  self.gram = nn.GramMatrix()\n",
    "  self.blend_weight = nil\n",
    "  self.G = nil\n",
    "  self.crit = nn.MSECriterion()\n",
    "end\n",
    "\n",
    "function StyleLoss:updateOutput(input)\n",
    "  self.G = self.gram:forward(input)\n",
    "  self.G:div(input:nElement())\n",
    "  if self.mode == 'capture' then\n",
    "    if self.blend_weight == nil then\n",
    "      self.target:resizeAs(self.G):copy(self.G)\n",
    "    elseif self.target:nElement() == 0 then\n",
    "      self.target:resizeAs(self.G):copy(self.G):mul(self.blend_weight)\n",
    "    else\n",
    "      self.target:add(self.blend_weight, self.G)\n",
    "    end\n",
    "  elseif self.mode == 'loss' then\n",
    "    self.loss = self.strength * self.crit:forward(self.G, self.target)\n",
    "  end\n",
    "  self.output = input\n",
    "  return self.output\n",
    "end\n",
    "\n",
    "function StyleLoss:updateGradInput(input, gradOutput)\n",
    "  if self.mode == 'loss' then\n",
    "    local dG = self.crit:backward(self.G, self.target)\n",
    "    dG:div(input:nElement())\n",
    "    self.gradInput = self.gram:backward(input, dG)\n",
    "    if self.normalize then\n",
    "      self.gradInput:div(torch.norm(self.gradInput, 1) + 1e-8)\n",
    "    end\n",
    "    self.gradInput:mul(self.strength)\n",
    "    self.gradInput:add(gradOutput)\n",
    "  else\n",
    "    self.gradInput = gradOutput\n",
    "  end\n",
    "  return self.gradInput\n",
    "end\n",
    "\n",
    "\n",
    "local TVLoss, parent = torch.class('nn.TVLoss', 'nn.Module')\n",
    "\n",
    "function TVLoss:__init(strength)\n",
    "  parent.__init(self)\n",
    "  self.strength = strength\n",
    "  self.x_diff = torch.Tensor()\n",
    "  self.y_diff = torch.Tensor()\n",
    "end\n",
    "\n",
    "function TVLoss:updateOutput(input)\n",
    "  self.output = input\n",
    "  return self.output\n",
    "end\n",
    "\n",
    "-- TV loss backward pass inspired by kaishengtai/neuralart\n",
    "function TVLoss:updateGradInput(input, gradOutput)\n",
    "  self.gradInput:resizeAs(input):zero()\n",
    "  local C, H, W = input:size(1), input:size(2), input:size(3)\n",
    "  self.x_diff:resize(3, H - 1, W - 1)\n",
    "  self.y_diff:resize(3, H - 1, W - 1)\n",
    "  self.x_diff:copy(input[{{}, {1, -2}, {1, -2}}])\n",
    "  self.x_diff:add(-1, input[{{}, {1, -2}, {2, -1}}])\n",
    "  self.y_diff:copy(input[{{}, {1, -2}, {1, -2}}])\n",
    "  self.y_diff:add(-1, input[{{}, {2, -1}, {1, -2}}])\n",
    "  self.gradInput[{{}, {1, -2}, {1, -2}}]:add(self.x_diff):add(self.y_diff)\n",
    "  self.gradInput[{{}, {1, -2}, {2, -1}}]:add(-1, self.x_diff)\n",
    "  self.gradInput[{{}, {2, -1}, {1, -2}}]:add(-1, self.y_diff)\n",
    "  self.gradInput:mul(self.strength)\n",
    "  self.gradInput:add(gradOutput)\n",
    "  return self.gradInput\n",
    "end\n",
    "\n",
    "\n",
    "local params = cmd:parse(arg)\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load neural-enhance/enhance.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"                          _              _                           \n",
    "  _ __   ___ _   _ _ __ __ _| |   ___ _ __ | |__   __ _ _ __   ___ ___  \n",
    " | '_ \\ / _ \\ | | | '__/ _` | |  / _ \\ '_ \\| '_ \\ / _` | '_ \\ / __/ _ \\ \n",
    " | | | |  __/ |_| | | | (_| | | |  __/ | | | | | | (_| | | | | (_|  __/ \n",
    " |_| |_|\\___|\\__,_|_|  \\__,_|_|  \\___|_| |_|_| |_|\\__,_|_| |_|\\___\\___| \n",
    "\n",
    "\"\"\"\n",
    "#\n",
    "# Copyright (c) 2016, Alex J. Champandard.\n",
    "#\n",
    "# Neural Enhance is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General\n",
    "# Public License version 3. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n",
    "# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
    "#\n",
    "\n",
    "__version__ = '0.3'\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import bz2\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import threading\n",
    "import collections\n",
    "\n",
    "\n",
    "# Configure all options first so we can later custom-load other libraries (Theano) based on device specified by user.\n",
    "parser = argparse.ArgumentParser(description='Generate a new image by applying style onto a content image.',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "add_arg = parser.add_argument\n",
    "add_arg('files',                nargs='*', default=[])\n",
    "add_arg('--zoom',               default=2, type=int,                help='Resolution increase factor for inference.')\n",
    "add_arg('--rendering-tile',     default=80, type=int,               help='Size of tiles used for rendering images.')\n",
    "add_arg('--rendering-overlap',  default=24, type=int,               help='Number of pixels padding around each tile.')\n",
    "add_arg('--rendering-histogram',default=False, action='store_true', help='Match color histogram of output to input.')\n",
    "add_arg('--type',               default='photo', type=str,          help='Name of the neural network to load/save.')\n",
    "add_arg('--model',              default='default', type=str,        help='Specific trained version of the model.')\n",
    "add_arg('--train',              default=False, type=str,            help='File pattern to load for training.')\n",
    "add_arg('--train-scales',       default=0, type=int,                help='Randomly resize images this many times.')\n",
    "add_arg('--train-blur',         default=None, type=int,             help='Sigma value for gaussian blur preprocess.')\n",
    "add_arg('--train-noise',        default=None, type=float,           help='Radius for preprocessing gaussian blur.')\n",
    "add_arg('--train-jpeg',         default=[], nargs='+', type=int,    help='JPEG compression level & range in preproc.')\n",
    "add_arg('--epochs',             default=10, type=int,               help='Total number of iterations in training.')\n",
    "add_arg('--epoch-size',         default=72, type=int,               help='Number of batches trained in an epoch.')\n",
    "add_arg('--save-every',         default=10, type=int,               help='Save generator after every training epoch.')\n",
    "add_arg('--batch-shape',        default=192, type=int,              help='Resolution of images in training batch.')\n",
    "add_arg('--batch-size',         default=15, type=int,               help='Number of images per training batch.')\n",
    "add_arg('--buffer-size',        default=1500, type=int,             help='Total image fragments kept in cache.')\n",
    "add_arg('--buffer-fraction',    default=5, type=int,                help='Fragments cached for each image loaded.')\n",
    "add_arg('--learning-rate',      default=1E-4, type=float,           help='Parameter for the ADAM optimizer.')\n",
    "add_arg('--learning-period',    default=75, type=int,               help='How often to decay the learning rate.')\n",
    "add_arg('--learning-decay',     default=0.5, type=float,            help='How much to decay the learning rate.')\n",
    "add_arg('--generator-upscale',  default=2, type=int,                help='Steps of 2x up-sampling as post-process.')\n",
    "add_arg('--generator-downscale',default=0, type=int,                help='Steps of 2x down-sampling as preprocess.')\n",
    "add_arg('--generator-filters',  default=[64], nargs='+', type=int,  help='Number of convolution units in network.')\n",
    "add_arg('--generator-blocks',   default=4, type=int,                help='Number of residual blocks per iteration.')\n",
    "add_arg('--generator-residual', default=2, type=int,                help='Number of layers in a residual block.')\n",
    "add_arg('--perceptual-layer',   default='conv2_2', type=str,        help='Which VGG layer to use as loss component.')\n",
    "add_arg('--perceptual-weight',  default=1e0, type=float,            help='Weight for VGG-layer perceptual loss.')\n",
    "add_arg('--discriminator-size', default=32, type=int,               help='Multiplier for number of filters in D.')\n",
    "add_arg('--smoothness-weight',  default=2e5, type=float,            help='Weight of the total-variation loss.')\n",
    "add_arg('--adversary-weight',   default=5e2, type=float,            help='Weight of adversarial loss compoment.')\n",
    "add_arg('--generator-start',    default=0, type=int,                help='Epoch count to start training generator.')\n",
    "add_arg('--discriminator-start',default=1, type=int,                help='Epoch count to update the discriminator.')\n",
    "add_arg('--adversarial-start',  default=2, type=int,                help='Epoch for generator to use discriminator.')\n",
    "add_arg('--device',             default='cpu', type=str,            help='Name of the CPU/GPU to use, for Theano.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Color coded output helps visualize the information a little better, plus it looks cool!\n",
    "class ansi:\n",
    "    WHITE = '\\033[0;97m'\n",
    "    WHITE_B = '\\033[1;97m'\n",
    "    YELLOW = '\\033[0;33m'\n",
    "    YELLOW_B = '\\033[1;33m'\n",
    "    RED = '\\033[0;31m'\n",
    "    RED_B = '\\033[1;31m'\n",
    "    BLUE = '\\033[0;94m'\n",
    "    BLUE_B = '\\033[1;94m'\n",
    "    CYAN = '\\033[0;36m'\n",
    "    CYAN_B = '\\033[1;36m'\n",
    "    ENDC = '\\033[0m'\n",
    "\n",
    "def error(message, *lines):\n",
    "    string = \"\\n{}ERROR: \" + message + \"{}\\n\" + \"\\n\".join(lines) + (\"{}\\n\" if lines else \"{}\")\n",
    "    print(string.format(ansi.RED_B, ansi.RED, ansi.ENDC))\n",
    "    sys.exit(-1)\n",
    "\n",
    "def warn(message, *lines):\n",
    "    string = \"\\n{}WARNING: \" + message + \"{}\\n\" + \"\\n\".join(lines) + \"{}\\n\"\n",
    "    print(string.format(ansi.YELLOW_B, ansi.YELLOW, ansi.ENDC))\n",
    "\n",
    "def extend(lst): return itertools.chain(lst, itertools.repeat(lst[-1]))\n",
    "\n",
    "print(\"\"\"{}   {}Super Resolution for images and videos powered by Deep Learning!{}\n",
    "  - Code licensed as AGPLv3, models under CC BY-NC-SA.{}\"\"\".format(ansi.CYAN_B, __doc__, ansi.CYAN, ansi.ENDC))\n",
    "\n",
    "# Load the underlying deep learning libraries based on the device specified.  If you specify THEANO_FLAGS manually,\n",
    "# the code assumes you know what you are doing and they are not overriden!\n",
    "os.environ.setdefault('THEANO_FLAGS', 'floatX=float32,device={},force_device=True,allow_gc=True,'\\\n",
    "                                      'print_active_device=False'.format(args.device))\n",
    "\n",
    "# Scientific & Imaging Libraries\n",
    "import numpy as np\n",
    "import scipy.ndimage, scipy.misc, PIL.Image\n",
    "\n",
    "# Numeric Computing (GPU)\n",
    "import theano, theano.tensor as T\n",
    "T.nnet.softminus = lambda x: x - T.nnet.softplus(x)\n",
    "\n",
    "# Support ansi colors in Windows too.\n",
    "if sys.platform == 'win32':\n",
    "    import colorama\n",
    "\n",
    "# Deep Learning Framework\n",
    "import lasagne\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer, Deconv2DLayer as DeconvLayer, Pool2DLayer as PoolLayer\n",
    "from lasagne.layers import InputLayer, ConcatLayer, ElemwiseSumLayer, batch_norm\n",
    "\n",
    "print('{}  - Using the device `{}` for neural computation.{}\\n'.format(ansi.CYAN, theano.config.device, ansi.ENDC))\n",
    "\n",
    "\n",
    "#======================================================================================================================\n",
    "# Image Processing\n",
    "#======================================================================================================================\n",
    "class DataLoader(threading.Thread):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DataLoader, self).__init__(daemon=True)\n",
    "        self.data_ready = threading.Event()\n",
    "        self.data_copied = threading.Event()\n",
    "\n",
    "        self.orig_shape, self.seed_shape = args.batch_shape, args.batch_shape // args.zoom\n",
    "\n",
    "        self.orig_buffer = np.zeros((args.buffer_size, 3, self.orig_shape, self.orig_shape), dtype=np.float32)\n",
    "        self.seed_buffer = np.zeros((args.buffer_size, 3, self.seed_shape, self.seed_shape), dtype=np.float32)\n",
    "        self.files = glob.glob(args.train)\n",
    "        if len(self.files) == 0:\n",
    "            error(\"There were no files found to train from searching for `{}`\".format(args.train),\n",
    "                  \"  - Try putting all your images in one folder and using `--train=data/*.jpg`\")\n",
    "\n",
    "        self.available = set(range(args.buffer_size))\n",
    "        self.ready = set()\n",
    "\n",
    "        self.cwd = os.getcwd()\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            random.shuffle(self.files)\n",
    "            for f in self.files:\n",
    "                self.add_to_buffer(f)\n",
    "\n",
    "    def add_to_buffer(self, f):\n",
    "        filename = os.path.join(self.cwd, f)\n",
    "        try:\n",
    "            orig = PIL.Image.open(filename).convert('RGB')\n",
    "            scale = 2 ** random.randint(0, args.train_scales)\n",
    "            if scale > 1 and all(s//scale >= args.batch_shape for s in orig.size):\n",
    "                orig = orig.resize((orig.size[0]//scale, orig.size[1]//scale), resample=PIL.Image.LANCZOS)\n",
    "            if any(s < args.batch_shape for s in orig.size):\n",
    "                raise ValueError('Image is too small for training with size {}'.format(orig.size))\n",
    "        except Exception as e:\n",
    "            warn('Could not load `{}` as image.'.format(filename),\n",
    "                 '  - Try fixing or removing the file before next run.')\n",
    "            self.files.remove(f)\n",
    "            return\n",
    "\n",
    "        seed = orig\n",
    "        if args.train_blur is not None:\n",
    "            seed = seed.filter(PIL.ImageFilter.GaussianBlur(radius=random.randint(0, args.train_blur*2)))\n",
    "        if args.zoom > 1:\n",
    "            seed = seed.resize((orig.size[0]//args.zoom, orig.size[1]//args.zoom), resample=PIL.Image.LANCZOS)\n",
    "        if len(args.train_jpeg) > 0:\n",
    "            buffer, rng = io.BytesIO(), args.train_jpeg[-1] if len(args.train_jpeg) > 1 else 15\n",
    "            seed.save(buffer, format='jpeg', quality=args.train_jpeg[0]+random.randrange(-rng, +rng))\n",
    "            seed = PIL.Image.open(buffer)\n",
    "\n",
    "        orig = scipy.misc.fromimage(orig).astype(np.float32)\n",
    "        seed = scipy.misc.fromimage(seed).astype(np.float32)\n",
    "\n",
    "        if args.train_noise is not None:\n",
    "            seed += scipy.random.normal(scale=args.train_noise, size=(seed.shape[0], seed.shape[1], 1))\n",
    "\n",
    "        for _ in range(seed.shape[0] * seed.shape[1] // (args.buffer_fraction * self.seed_shape ** 2)):\n",
    "            h = random.randint(0, seed.shape[0] - self.seed_shape)\n",
    "            w = random.randint(0, seed.shape[1] - self.seed_shape)\n",
    "            seed_chunk = seed[h:h+self.seed_shape, w:w+self.seed_shape]\n",
    "            h, w = h * args.zoom, w * args.zoom\n",
    "            orig_chunk = orig[h:h+self.orig_shape, w:w+self.orig_shape]\n",
    "\n",
    "            while len(self.available) == 0:\n",
    "                self.data_copied.wait()\n",
    "                self.data_copied.clear()\n",
    "\n",
    "            i = self.available.pop()\n",
    "            self.orig_buffer[i] = np.transpose(orig_chunk.astype(np.float32) / 255.0 - 0.5, (2, 0, 1))\n",
    "            self.seed_buffer[i] = np.transpose(seed_chunk.astype(np.float32) / 255.0 - 0.5, (2, 0, 1))\n",
    "            self.ready.add(i)\n",
    "\n",
    "            if len(self.ready) >= args.batch_size:\n",
    "                self.data_ready.set()\n",
    "\n",
    "    def copy(self, origs_out, seeds_out):\n",
    "        self.data_ready.wait()\n",
    "        self.data_ready.clear()\n",
    "\n",
    "        for i, j in enumerate(random.sample(self.ready, args.batch_size)):\n",
    "            origs_out[i] = self.orig_buffer[j]\n",
    "            seeds_out[i] = self.seed_buffer[j]\n",
    "            self.available.add(j)\n",
    "        self.data_copied.set()\n",
    "\n",
    "\n",
    "#======================================================================================================================\n",
    "# Convolution Networks\n",
    "#======================================================================================================================\n",
    "\n",
    "class SubpixelReshuffleLayer(lasagne.layers.Layer):\n",
    "    \"\"\"Based on the code by ajbrock: https://github.com/ajbrock/Neural-Photo-Editor/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, incoming, channels, upscale, **kwargs):\n",
    "        super(SubpixelReshuffleLayer, self).__init__(incoming, **kwargs)\n",
    "        self.upscale = upscale\n",
    "        self.channels = channels\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        def up(d): return self.upscale * d if d else d\n",
    "        return (input_shape[0], self.channels, up(input_shape[2]), up(input_shape[3]))\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False, **kwargs):\n",
    "        out, r = T.zeros(self.get_output_shape_for(input.shape)), self.upscale\n",
    "        for y, x in itertools.product(range(r), repeat=2):\n",
    "            out=T.inc_subtensor(out[:,:,y::r,x::r], input[:,r*y+x::r*r,:,:])\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.network = collections.OrderedDict()\n",
    "        self.network['img'] = InputLayer((None, 3, None, None))\n",
    "        self.network['seed'] = InputLayer((None, 3, None, None))\n",
    "\n",
    "        config, params = self.load_model()\n",
    "        self.setup_generator(self.last_layer(), config)\n",
    "\n",
    "        if args.train:\n",
    "            concatenated = lasagne.layers.ConcatLayer([self.network['img'], self.network['out']], axis=0)\n",
    "            self.setup_perceptual(concatenated)\n",
    "            self.load_perceptual()\n",
    "            self.setup_discriminator()\n",
    "        self.load_generator(params)\n",
    "        self.compile()\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    # Network Configuration\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def last_layer(self):\n",
    "        return list(self.network.values())[-1]\n",
    "\n",
    "    def make_layer(self, name, input, units, filter_size=(3,3), stride=(1,1), pad=(1,1), alpha=0.25):\n",
    "        conv = ConvLayer(input, units, filter_size, stride=stride, pad=pad, nonlinearity=None)\n",
    "        prelu = lasagne.layers.ParametricRectifierLayer(conv, alpha=lasagne.init.Constant(alpha))\n",
    "        self.network[name+'x'] = conv\n",
    "        self.network[name+'>'] = prelu\n",
    "        return prelu\n",
    "\n",
    "    def make_block(self, name, input, units):\n",
    "        self.make_layer(name+'-A', input, units, alpha=0.1)\n",
    "        # self.make_layer(name+'-B', self.last_layer(), units, alpha=1.0)\n",
    "        return ElemwiseSumLayer([input, self.last_layer()]) if args.generator_residual else self.last_layer()\n",
    "\n",
    "    def setup_generator(self, input, config):\n",
    "        for k, v in config.items(): setattr(args, k, v)\n",
    "        args.zoom = 2**(args.generator_upscale - args.generator_downscale)\n",
    "\n",
    "        units_iter = extend(args.generator_filters)\n",
    "        units = next(units_iter)\n",
    "        self.make_layer('iter.0', input, units, filter_size=(7,7), pad=(3,3))\n",
    "\n",
    "        for i in range(0, args.generator_downscale):\n",
    "            self.make_layer('downscale%i'%i, self.last_layer(), next(units_iter), filter_size=(4,4), stride=(2,2))\n",
    "\n",
    "        units = next(units_iter)\n",
    "        for i in range(0, args.generator_blocks):\n",
    "            self.make_block('iter.%i'%(i+1), self.last_layer(), units)\n",
    "\n",
    "        for i in range(0, args.generator_upscale):\n",
    "            u = next(units_iter)\n",
    "            self.make_layer('upscale%i.2'%i, self.last_layer(), u*4)\n",
    "            self.network['upscale%i.1'%i] = SubpixelReshuffleLayer(self.last_layer(), u, 2)\n",
    "\n",
    "        self.network['out'] = ConvLayer(self.last_layer(), 3, filter_size=(7,7), pad=(3,3), nonlinearity=None)\n",
    "\n",
    "    def setup_perceptual(self, input):\n",
    "        \"\"\"Use lasagne to create a network of convolution layers using pre-trained VGG19 weights.\n",
    "        \"\"\"\n",
    "        offset = np.array([103.939, 116.779, 123.680], dtype=np.float32).reshape((1,3,1,1))\n",
    "        self.network['percept'] = lasagne.layers.NonlinearityLayer(input, lambda x: ((x+0.5)*255.0) - offset)\n",
    "\n",
    "        self.network['mse'] = self.network['percept']\n",
    "        self.network['conv1_1'] = ConvLayer(self.network['percept'], 64, 3, pad=1)\n",
    "        self.network['conv1_2'] = ConvLayer(self.network['conv1_1'], 64, 3, pad=1)\n",
    "        self.network['pool1']   = PoolLayer(self.network['conv1_2'], 2, mode='max')\n",
    "        self.network['conv2_1'] = ConvLayer(self.network['pool1'],   128, 3, pad=1)\n",
    "        self.network['conv2_2'] = ConvLayer(self.network['conv2_1'], 128, 3, pad=1)\n",
    "        self.network['pool2']   = PoolLayer(self.network['conv2_2'], 2, mode='max')\n",
    "        self.network['conv3_1'] = ConvLayer(self.network['pool2'],   256, 3, pad=1)\n",
    "        self.network['conv3_2'] = ConvLayer(self.network['conv3_1'], 256, 3, pad=1)\n",
    "        self.network['conv3_3'] = ConvLayer(self.network['conv3_2'], 256, 3, pad=1)\n",
    "        self.network['conv3_4'] = ConvLayer(self.network['conv3_3'], 256, 3, pad=1)\n",
    "        self.network['pool3']   = PoolLayer(self.network['conv3_4'], 2, mode='max')\n",
    "        self.network['conv4_1'] = ConvLayer(self.network['pool3'],   512, 3, pad=1)\n",
    "        self.network['conv4_2'] = ConvLayer(self.network['conv4_1'], 512, 3, pad=1)\n",
    "        self.network['conv4_3'] = ConvLayer(self.network['conv4_2'], 512, 3, pad=1)\n",
    "        self.network['conv4_4'] = ConvLayer(self.network['conv4_3'], 512, 3, pad=1)\n",
    "        self.network['pool4']   = PoolLayer(self.network['conv4_4'], 2, mode='max')\n",
    "        self.network['conv5_1'] = ConvLayer(self.network['pool4'],   512, 3, pad=1)\n",
    "        self.network['conv5_2'] = ConvLayer(self.network['conv5_1'], 512, 3, pad=1)\n",
    "        self.network['conv5_3'] = ConvLayer(self.network['conv5_2'], 512, 3, pad=1)\n",
    "        self.network['conv5_4'] = ConvLayer(self.network['conv5_3'], 512, 3, pad=1)\n",
    "\n",
    "    def setup_discriminator(self):\n",
    "        c = args.discriminator_size\n",
    "        self.make_layer('disc1.1', batch_norm(self.network['conv1_2']), 1*c, filter_size=(5,5), stride=(2,2), pad=(2,2))\n",
    "        self.make_layer('disc1.2', self.last_layer(), 1*c, filter_size=(5,5), stride=(2,2), pad=(2,2))\n",
    "        self.make_layer('disc2', batch_norm(self.network['conv2_2']), 2*c, filter_size=(5,5), stride=(2,2), pad=(2,2))\n",
    "        self.make_layer('disc3', batch_norm(self.network['conv3_2']), 3*c, filter_size=(3,3), stride=(1,1), pad=(1,1))\n",
    "        hypercolumn = ConcatLayer([self.network['disc1.2>'], self.network['disc2>'], self.network['disc3>']])\n",
    "        self.make_layer('disc4', hypercolumn, 4*c, filter_size=(1,1), stride=(1,1), pad=(0,0))\n",
    "        self.make_layer('disc5', self.last_layer(), 3*c, filter_size=(3,3), stride=(2,2))\n",
    "        self.make_layer('disc6', self.last_layer(), 2*c, filter_size=(1,1), stride=(1,1), pad=(0,0))\n",
    "        self.network['disc'] = batch_norm(ConvLayer(self.last_layer(), 1, filter_size=(1,1),\n",
    "                                                    nonlinearity=lasagne.nonlinearities.linear))\n",
    "\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    # Input / Output\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def load_perceptual(self):\n",
    "        \"\"\"Open the serialized parameters from a pre-trained network, and load them into the model created.\n",
    "        \"\"\"\n",
    "        vgg19_file = os.path.join(os.path.dirname(__file__), 'vgg19_conv.pkl.bz2')\n",
    "        if not os.path.exists(vgg19_file):\n",
    "            error(\"Model file with pre-trained convolution layers not found. Download here...\",\n",
    "                  \"https://github.com/alexjc/neural-doodle/releases/download/v0.0/vgg19_conv.pkl.bz2\")\n",
    "\n",
    "        data = pickle.load(bz2.open(vgg19_file, 'rb'))\n",
    "        layers = lasagne.layers.get_all_layers(self.last_layer(), treat_as_input=[self.network['percept']])\n",
    "        for p, d in zip(itertools.chain(*[l.get_params() for l in layers]), data): p.set_value(d)\n",
    "\n",
    "    def list_generator_layers(self):\n",
    "        for l in lasagne.layers.get_all_layers(self.network['out'], treat_as_input=[self.network['img']]):\n",
    "            if not l.get_params(): continue\n",
    "            name = list(self.network.keys())[list(self.network.values()).index(l)]\n",
    "            yield (name, l)\n",
    "\n",
    "    def get_filename(self, absolute=False):\n",
    "        filename = 'ne%ix-%s-%s-%s.pkl.bz2' % (args.zoom, args.type, args.model, __version__)\n",
    "        return os.path.join(os.path.dirname(__file__), filename) if absolute else filename\n",
    "\n",
    "    def save_generator(self):\n",
    "        def cast(p): return p.get_value().astype(np.float16)\n",
    "        params = {k: [cast(p) for p in l.get_params()] for (k, l) in self.list_generator_layers()}\n",
    "        config = {k: getattr(args, k) for k in ['generator_blocks', 'generator_residual', 'generator_filters'] + \\\n",
    "                                               ['generator_upscale', 'generator_downscale']}\n",
    "        \n",
    "        pickle.dump((config, params), bz2.open(self.get_filename(absolute=True), 'wb'))\n",
    "        print('  - Saved model as `{}` after training.'.format(self.get_filename()))\n",
    "\n",
    "    def load_model(self):\n",
    "        if not os.path.exists(self.get_filename(absolute=True)):\n",
    "            if args.train: return {}, {}\n",
    "            error(\"Model file with pre-trained convolution layers not found. Download it here...\",\n",
    "                  \"https://github.com/alexjc/neural-enhance/releases/download/v%s/%s\"%(__version__, self.get_filename()))\n",
    "        print('  - Loaded file `{}` with trained model.'.format(self.get_filename()))\n",
    "        return pickle.load(bz2.open(self.get_filename(absolute=True), 'rb'))\n",
    "\n",
    "    def load_generator(self, params):\n",
    "        if len(params) == 0: return\n",
    "        for k, l in self.list_generator_layers():\n",
    "            assert k in params, \"Couldn't find layer `%s` in loaded model.'\" % k\n",
    "            assert len(l.get_params()) == len(params[k]), \"Mismatch in types of layers.\"\n",
    "            for p, v in zip(l.get_params(), params[k]):\n",
    "                assert v.shape == p.get_value().shape, \"Mismatch in number of parameters for layer {}.\".format(k)\n",
    "                p.set_value(v.astype(np.float32))\n",
    "\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "    # Training & Loss Functions\n",
    "    #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def loss_perceptual(self, p):\n",
    "        return lasagne.objectives.squared_error(p[:args.batch_size], p[args.batch_size:]).mean()\n",
    "\n",
    "    def loss_total_variation(self, x):\n",
    "        return T.mean(((x[:,:,:-1,:-1] - x[:,:,1:,:-1])**2 + (x[:,:,:-1,:-1] - x[:,:,:-1,1:])**2)**1.25)\n",
    "\n",
    "    def loss_adversarial(self, d):\n",
    "        return T.mean(1.0 - T.nnet.softminus(d[args.batch_size:]))\n",
    "\n",
    "    def loss_discriminator(self, d):\n",
    "        return T.mean(T.nnet.softminus(d[args.batch_size:]) - T.nnet.softplus(d[:args.batch_size]))\n",
    "\n",
    "    def compile(self):\n",
    "        # Helper function for rendering test images during training, or standalone inference mode.\n",
    "        input_tensor, seed_tensor = T.tensor4(), T.tensor4()\n",
    "        input_layers = {self.network['img']: input_tensor, self.network['seed']: seed_tensor}\n",
    "        output = lasagne.layers.get_output([self.network[k] for k in ['seed','out']], input_layers, deterministic=True)\n",
    "        self.predict = theano.function([seed_tensor], output)\n",
    "\n",
    "        if not args.train: return\n",
    "\n",
    "        output_layers = [self.network['out'], self.network[args.perceptual_layer], self.network['disc']]\n",
    "        gen_out, percept_out, disc_out = lasagne.layers.get_output(output_layers, input_layers, deterministic=False)\n",
    "\n",
    "        # Generator loss function, parameters and updates.\n",
    "        self.gen_lr = theano.shared(np.array(0.0, dtype=theano.config.floatX))\n",
    "        self.adversary_weight = theano.shared(np.array(0.0, dtype=theano.config.floatX))\n",
    "        gen_losses = [self.loss_perceptual(percept_out) * args.perceptual_weight,\n",
    "                      self.loss_total_variation(gen_out) * args.smoothness_weight,\n",
    "                      self.loss_adversarial(disc_out) * self.adversary_weight]\n",
    "        gen_params = lasagne.layers.get_all_params(self.network['out'], trainable=True)\n",
    "        print('  - {} tensors learned for generator.'.format(len(gen_params)))\n",
    "        gen_updates = lasagne.updates.adam(sum(gen_losses, 0.0), gen_params, learning_rate=self.gen_lr)\n",
    "\n",
    "        # Discriminator loss function, parameters and updates.\n",
    "        self.disc_lr = theano.shared(np.array(0.0, dtype=theano.config.floatX))\n",
    "        disc_losses = [self.loss_discriminator(disc_out)]\n",
    "        disc_params = list(itertools.chain(*[l.get_params() for k, l in self.network.items() if 'disc' in k]))\n",
    "        print('  - {} tensors learned for discriminator.'.format(len(disc_params)))\n",
    "        grads = [g.clip(-5.0, +5.0) for g in T.grad(sum(disc_losses, 0.0), disc_params)]\n",
    "        disc_updates = lasagne.updates.adam(grads, disc_params, learning_rate=self.disc_lr)\n",
    "\n",
    "        # Combined Theano function for updating both generator and discriminator at the same time.\n",
    "        updates = collections.OrderedDict(list(gen_updates.items()) + list(disc_updates.items()))\n",
    "        self.fit = theano.function([input_tensor, seed_tensor], gen_losses + [disc_out.mean(axis=(1,2,3))], updates=updates)\n",
    "\n",
    "\n",
    "\n",
    "class NeuralEnhancer(object):\n",
    "\n",
    "    def __init__(self, loader):\n",
    "        if args.train:\n",
    "            print('{}Training {} epochs on random image sections with batch size {}.{}'\\\n",
    "                  .format(ansi.BLUE_B, args.epochs, args.batch_size, ansi.BLUE))\n",
    "        else:\n",
    "            if len(args.files) == 0: error(\"Specify the image(s) to enhance on the command-line.\")\n",
    "            print('{}Enhancing {} image(s) specified on the command-line.{}'\\\n",
    "                  .format(ansi.BLUE_B, len(args.files), ansi.BLUE))\n",
    "\n",
    "        self.thread = DataLoader() if loader else None\n",
    "        self.model = Model()\n",
    "\n",
    "        print('{}'.format(ansi.ENDC))\n",
    "\n",
    "    def imsave(self, fn, img):\n",
    "        scipy.misc.toimage(np.transpose(img + 0.5, (1, 2, 0)).clip(0.0, 1.0) * 255.0, cmin=0, cmax=255).save(fn)\n",
    "\n",
    "    def show_progress(self, orign, scald, repro):\n",
    "        os.makedirs('valid', exist_ok=True)\n",
    "        for i in range(args.batch_size):\n",
    "            self.imsave('valid/%s_%03i_origin.png' % (args.model, i), orign[i])\n",
    "            self.imsave('valid/%s_%03i_pixels.png' % (args.model, i), scald[i])\n",
    "            self.imsave('valid/%s_%03i_reprod.png' % (args.model, i), repro[i])\n",
    "\n",
    "    def decay_learning_rate(self):\n",
    "        l_r, t_cur = args.learning_rate, 0\n",
    "\n",
    "        while True:\n",
    "            yield l_r\n",
    "            t_cur += 1\n",
    "            if t_cur % args.learning_period == 0: l_r *= args.learning_decay\n",
    "\n",
    "    def train(self):\n",
    "        seed_size = args.batch_shape // args.zoom\n",
    "        images = np.zeros((args.batch_size, 3, args.batch_shape, args.batch_shape), dtype=np.float32)\n",
    "        seeds = np.zeros((args.batch_size, 3, seed_size, seed_size), dtype=np.float32)\n",
    "        learning_rate = self.decay_learning_rate()\n",
    "        try:\n",
    "            average, start = None, time.time()\n",
    "            for epoch in range(args.epochs):\n",
    "                total, stats = None, None\n",
    "                l_r = next(learning_rate)\n",
    "                if epoch >= args.generator_start: self.model.gen_lr.set_value(l_r)\n",
    "                if epoch >= args.discriminator_start: self.model.disc_lr.set_value(l_r)\n",
    "\n",
    "                for _ in range(args.epoch_size):\n",
    "                    self.thread.copy(images, seeds)\n",
    "                    output = self.model.fit(images, seeds)\n",
    "                    losses = np.array(output[:3], dtype=np.float32)\n",
    "                    stats = (stats + output[3]) if stats is not None else output[3]\n",
    "                    total = total + losses if total is not None else losses\n",
    "                    l = np.sum(losses)\n",
    "                    assert not np.isnan(losses).any()\n",
    "                    average = l if average is None else average * 0.95 + 0.05 * l\n",
    "                    print('↑' if l > average else '↓', end='', flush=True)\n",
    "\n",
    "                scald, repro = self.model.predict(seeds)\n",
    "                self.show_progress(images, scald, repro)\n",
    "                total /= args.epoch_size\n",
    "                stats /= args.epoch_size\n",
    "                totals, labels = [sum(total)] + list(total), ['total', 'prcpt', 'smthn', 'advrs']\n",
    "                gen_info = ['{}{}{}={:4.2e}'.format(ansi.WHITE_B, k, ansi.ENDC, v) for k, v in zip(labels, totals)]\n",
    "                print('\\rEpoch #{} at {:4.1f}s, lr={:4.2e}{}'.format(epoch+1, time.time()-start, l_r, ' '*(args.epoch_size-30)))\n",
    "                print('  - generator {}'.format(' '.join(gen_info)))\n",
    "\n",
    "                real, fake = stats[:args.batch_size], stats[args.batch_size:]\n",
    "                print('  - discriminator', real.mean(), len(np.where(real > 0.5)[0]),\n",
    "                                           fake.mean(), len(np.where(fake < -0.5)[0]))\n",
    "                if epoch == args.adversarial_start-1:\n",
    "                    print('  - generator now optimizing against discriminator.')\n",
    "                    self.model.adversary_weight.set_value(args.adversary_weight)\n",
    "                    running = None\n",
    "                if (epoch+1) % args.save_every == 0:\n",
    "                    print('  - saving current generator layers to disk...')\n",
    "                    self.model.save_generator()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        print('\\n{}Trained {}x super-resolution for {} epochs.{}'\\\n",
    "                .format(ansi.CYAN_B, args.zoom, epoch+1, ansi.CYAN))\n",
    "        self.model.save_generator()\n",
    "        print(ansi.ENDC)\n",
    "\n",
    "    def match_histograms(self, A, B, rng=(0.0, 255.0), bins=64):\n",
    "        (Ha, Xa), (Hb, Xb) = [np.histogram(i, bins=bins, range=rng, density=True) for i in [A, B]]\n",
    "        X = np.linspace(rng[0], rng[1], bins, endpoint=True)\n",
    "        Hpa, Hpb = [np.cumsum(i) * (rng[1] - rng[0]) ** 2 / float(bins) for i in [Ha, Hb]]\n",
    "        inv_Ha = scipy.interpolate.interp1d(X, Hpa, bounds_error=False, fill_value='extrapolate')\n",
    "        map_Hb = scipy.interpolate.interp1d(Hpb, X, bounds_error=False, fill_value='extrapolate')\n",
    "        return map_Hb(inv_Ha(A).clip(0.0, 255.0))\n",
    "\n",
    "    def process(self, original):\n",
    "        # Snap the image to a shape that's compatible with the generator (2x, 4x)\n",
    "        s = 2 ** max(args.generator_upscale, args.generator_downscale)\n",
    "        by, bx = original.shape[0] % s, original.shape[1] % s\n",
    "        original = original[by-by//2:original.shape[0]-by//2,bx-bx//2:original.shape[1]-bx//2,:]\n",
    "\n",
    "        # Prepare paded input image as well as output buffer of zoomed size.\n",
    "        s, p, z = args.rendering_tile, args.rendering_overlap, args.zoom\n",
    "        image = np.pad(original, ((p, p), (p, p), (0, 0)), mode='reflect')\n",
    "        output = np.zeros((original.shape[0] * z, original.shape[1] * z, 3), dtype=np.float32)\n",
    "\n",
    "        # Iterate through the tile coordinates and pass them through the network.\n",
    "        for y, x in itertools.product(range(0, original.shape[0], s), range(0, original.shape[1], s)):\n",
    "            img = np.transpose(image[y:y+p*2+s,x:x+p*2+s,:] / 255.0 - 0.5, (2, 0, 1))[np.newaxis].astype(np.float32)\n",
    "            *_, repro = self.model.predict(img)\n",
    "            output[y*z:(y+s)*z,x*z:(x+s)*z,:] = np.transpose(repro[0] + 0.5, (1, 2, 0))[p*z:-p*z,p*z:-p*z,:]\n",
    "            print('.', end='', flush=True)\n",
    "        output = output.clip(0.0, 1.0) * 255.0\n",
    "\n",
    "        # Match color histograms if the user specified this option.\n",
    "        if args.rendering_histogram:\n",
    "            for i in range(3):\n",
    "                output[:,:,i] = self.match_histograms(output[:,:,i], original[:,:,i])\n",
    "\n",
    "        return scipy.misc.toimage(output, cmin=0, cmax=255)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if args.train:\n",
    "        args.zoom = 2**(args.generator_upscale - args.generator_downscale)\n",
    "        enhancer = NeuralEnhancer(loader=True)\n",
    "        enhancer.train()\n",
    "    else:\n",
    "        enhancer = NeuralEnhancer(loader=False)\n",
    "        for filename in args.files:\n",
    "            print(filename, end=' ')\n",
    "            img = scipy.ndimage.imread(filename, mode='RGB')\n",
    "            out = enhancer.process(img)\n",
    "            out.save(os.path.splitext(filename)[0]+'_ne%ix.png' % args.zoom)\n",
    "            print(flush=True)\n",
    "        print(ansi.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}